{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks: Multi Layer Perceptron\n",
    "## In this lab we will learn about Multi-layer Perceptrons\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "# From sklearn we will use the implementations of the Multi-layer perceptron\n",
    "from sklearn import cluster\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import binarize\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.datasets.mldata import fetch_mldata\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_digits\n",
    "from keras.datasets import mnist\n",
    "\n",
    "\n",
    "# We will also use different metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 ... 5 6 8]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[7 2 1 ... 4 5 6]\n",
      "[1 0 0 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(y_train)\n",
    "y_train = np.array([1 if x > 4 else 0 for x in y_train])\n",
    "print(y_train)\n",
    "print(y_test)\n",
    "y_test = np.array([1 if x > 4 else 0 for x in y_test])\n",
    "print(y_test)\n",
    "\n",
    "X_train = np.reshape(X_train, (60000, 28*28))  # What are these two lines doing?\n",
    "X_test = np.reshape(X_test, (10000, 28*28))\n",
    "train_n_samples = 60000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_coefs(clf):\n",
    "    \"\"\"\n",
    "    clf must be the instanced (and trained) classifier\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(4, 4)\n",
    "    # use global min / max to ensure all weights are shown on the same scale\n",
    "    vmin, vmax = mlp.coefs_[0].min(), mlp.coefs_[0].max()\n",
    "    for coef, ax in zip(mlp.coefs_[0].T, axes.ravel()):\n",
    "        ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5 * vmin, vmax=.5 * vmax)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Now that we know how to create a MLP the easy way, we are going to give one step further, and design a MLP using tensorflow from scratch.\n",
    "\n",
    "Suggestion: Use the same technique used in the tensorflow notebook to design te MLP. Following the notation used in that notebook (in which we formalized the linear models, y = W*X + b), design a one-hidden-layer MLP, which can be defined as y = (W1*X + b1)*W2 + b2. The rest of the tensorflow components (loss function, optimizer, ...) need no changes to work with this model. The MLP will have one single hidden layer, of 100 neurons.\n",
    "\n",
    "Take into account that this model will be used for **binary classification** (this affects the loss function that needs to be optimized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "mini_batch_size = 100\n",
    "\n",
    "# Declare the placeholders\n",
    "\n",
    "\n",
    "\n",
    "# Declare the rest of variables\n",
    "\n",
    "\n",
    "\n",
    "# Create the model y = (W1*X + b1)*W2 + b2 and loss function\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "\n",
    "# Declare an optimizer\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "training_epochs = 40000\n",
    "display_step = 500\n",
    "perm = np.random.permutation(train_n_samples)\n",
    "\n",
    "\n",
    "n_batch = train_n_samples // mini_batch_size + (train_n_samples % mini_batch_size != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Use the designed MLP to learn a **classifier** for the MNIST dataset that was used in the example. You can reuse the training algorithm used in the tensorflow notebook to train this model.\n",
    "\n",
    "Optional: Along with the error, display the accuracy of the model while training it. For that, you will have to compute it each time (inside the display_step *if*), or use a tensorflow operation (which will be a more optimized and sophisticated approach). For the first option, you will have to compute the predictions and compare them with the actual labels. For the second option, you will have to compute a \"secondary loss function\" which is not used for optimization, but only to show results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0500 cost= 30314.716796875\n",
      "Epoch: 1000 cost= 16630.796875000\n",
      "Epoch: 1500 cost= 7495.603515625\n",
      "Epoch: 2000 cost= 9143.077148438\n",
      "Epoch: 2500 cost= 4318.079101562\n",
      "Epoch: 3000 cost= 7912.834960938\n",
      "Epoch: 3500 cost= 7438.613769531\n",
      "Epoch: 4000 cost= 6567.265136719\n",
      "Epoch: 4500 cost= 2717.686035156\n",
      "Epoch: 5000 cost= 4734.321777344\n",
      "Epoch: 5500 cost= 2321.935302734\n",
      "Epoch: 6000 cost= 3813.561035156\n",
      "Epoch: 6500 cost= 4045.684326172\n",
      "Epoch: 7000 cost= 3392.591308594\n",
      "Epoch: 7500 cost= 1636.912841797\n",
      "Epoch: 8000 cost= 2671.666259766\n",
      "Epoch: 8500 cost= 1197.667236328\n",
      "Epoch: 9000 cost= 2026.114013672\n",
      "Epoch: 9500 cost= 2538.968017578\n",
      "Epoch: 10000 cost= 1859.041259766\n",
      "Epoch: 10500 cost= 1050.409912109\n",
      "Epoch: 11000 cost= 1622.656738281\n",
      "Epoch: 11500 cost= 658.687500000\n",
      "Epoch: 12000 cost= 1118.507202148\n",
      "Epoch: 12500 cost= 1590.380371094\n",
      "Epoch: 13000 cost= 1029.534423828\n",
      "Epoch: 13500 cost= 739.396240234\n",
      "Epoch: 14000 cost= 963.368652344\n",
      "Epoch: 14500 cost= 348.022857666\n",
      "Epoch: 15000 cost= 631.003967285\n",
      "Epoch: 15500 cost= 951.425476074\n",
      "Epoch: 16000 cost= 633.586425781\n",
      "Epoch: 16500 cost= 479.181732178\n",
      "Epoch: 17000 cost= 596.830139160\n",
      "Epoch: 17500 cost= 203.129791260\n",
      "Epoch: 18000 cost= 384.163116455\n",
      "Epoch: 18500 cost= 553.413452148\n",
      "Epoch: 19000 cost= 578.491821289\n",
      "Epoch: 19500 cost= 353.401062012\n",
      "Epoch: 20000 cost= 349.265625000\n",
      "Epoch: 20500 cost= 132.531829834\n",
      "Epoch: 21000 cost= 212.196365356\n",
      "Epoch: 21500 cost= 364.647003174\n",
      "Epoch: 22000 cost= 424.690734863\n",
      "Epoch: 22500 cost= 285.767395020\n",
      "Epoch: 23000 cost= 244.864334106\n",
      "Epoch: 23500 cost= 92.304908752\n",
      "Epoch: 24000 cost= 146.065124512\n",
      "Epoch: 24500 cost= 197.576644897\n",
      "Epoch: 25000 cost= 185.539291382\n",
      "Epoch: 25500 cost= 156.097702026\n",
      "Epoch: 26000 cost= 150.830169678\n",
      "Epoch: 26500 cost= 74.132705688\n",
      "Epoch: 27000 cost= 97.054817200\n",
      "Epoch: 27500 cost= 137.598190308\n",
      "Epoch: 28000 cost= 85.882179260\n",
      "Epoch: 28500 cost= 124.303909302\n",
      "Epoch: 29000 cost= 96.180282593\n",
      "Epoch: 29500 cost= 43.454658508\n",
      "Epoch: 30000 cost= 71.243385315\n",
      "Epoch: 30500 cost= 99.170372009\n",
      "Epoch: 31000 cost= 67.893356323\n",
      "Epoch: 31500 cost= 89.574279785\n",
      "Epoch: 32000 cost= 64.741065979\n",
      "Epoch: 32500 cost= 48.815814972\n",
      "Epoch: 33000 cost= 53.963916779\n",
      "Epoch: 33500 cost= 38.003345490\n",
      "Epoch: 34000 cost= 50.138381958\n",
      "Epoch: 34500 cost= 61.999004364\n",
      "Epoch: 35000 cost= 47.460060120\n",
      "Epoch: 35500 cost= 59.231758118\n",
      "Epoch: 36000 cost= 48.221012115\n",
      "Epoch: 36500 cost= 30.315542221\n",
      "Epoch: 37000 cost= 45.273143768\n",
      "Epoch: 37500 cost= 58.056049347\n",
      "Epoch: 38000 cost= 45.362625122\n",
      "Epoch: 38500 cost= 64.423049927\n",
      "Epoch: 39000 cost= 26.111265182\n",
      "Epoch: 39500 cost= 22.309764862\n",
      "Epoch: 40000 cost= 48.279518127\n",
      "Training loss= 57.168526 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        i_batch = (epoch % n_batch)*mini_batch_size\n",
    "        batch = X_train[i_batch:i_batch+mini_batch_size], y_train[i_batch:i_batch+mini_batch_size]\n",
    "        # run optimization operation\n",
    "        \n",
    "        if (epoch+1) % display_step == 0:\n",
    "            err, p = sess.run((loss, tf.nn.sigmoid(prediction)), feed_dict={X: batch[0],  Y: np.reshape(batch[1], (-1,1))})\n",
    "            \n",
    "            # Compute and show the accuracy of the model\n",
    "            \n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(err))\n",
    "\n",
    "    training_loss = sess.run(loss, feed_dict={X: X_test, Y: np.reshape(y_test, (-1,1))})\n",
    "    print(\"Training loss=\", training_loss, '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 \n",
    "\n",
    "Modify the visualize_coefs function so that it can show some of the coefficients in the first layer of the learned network. Instead of accessing the coefficients computed in the mlp object, you will have to show the coefficients computed in the weight variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until this point, we have let tensorflow optimize our models (both the simple ones that we did in the last lab, and the more complex MLP in this one) without much knowledge about the numerical computations that the optimizer did. Now that we know more about the backpropagation error, we are going to dig deeper into tensorflow, and see what operations are being performed in each step of the model training.\n",
    "\n",
    "To know how the values change in each learning iteration, we are going to divide the optimizing operation in two halves, gradient computing, and gradient application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = adam.compute_gradients(loss, var_list=variable_list)\n",
    "application = adam.apply_gradients(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous cell, we have set a tensorflow operation that computes the gradients for all the variables, and then, we have defined another operation that applies these gradients to the variable.\n",
    "\n",
    "The *grads* object contains the current value of the variables, and the gradient of these variables.\n",
    "Using these two tensorflow operations results in the same outcome as using the minimize(loss) function of the optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0500 cost= 35199.300781250\n",
      "Epoch: 1000 cost= 15801.134765625\n",
      "Epoch: 1500 cost= 14258.865234375\n",
      "Epoch: 2000 cost= 15981.629882812\n",
      "Epoch: 2500 cost= 5333.068847656\n",
      "Epoch: 3000 cost= 4249.783691406\n",
      "Epoch: 3500 cost= 5076.312011719\n",
      "Epoch: 4000 cost= 3628.100097656\n",
      "Epoch: 4500 cost= 4364.167480469\n",
      "Epoch: 5000 cost= 5796.270019531\n",
      "Epoch: 5500 cost= 1905.759399414\n",
      "Epoch: 6000 cost= 2384.908203125\n",
      "Epoch: 6500 cost= 2428.159423828\n",
      "Epoch: 7000 cost= 1798.767822266\n",
      "Epoch: 7500 cost= 2060.764648438\n",
      "Epoch: 8000 cost= 2803.760986328\n",
      "Epoch: 8500 cost= 984.450683594\n",
      "Epoch: 9000 cost= 1578.440185547\n",
      "Epoch: 9500 cost= 1150.056762695\n",
      "Epoch: 10000 cost= 1133.869262695\n",
      "Epoch: 10500 cost= 1162.719238281\n",
      "Epoch: 11000 cost= 1608.653442383\n",
      "Epoch: 11500 cost= 563.721862793\n",
      "Epoch: 12000 cost= 956.941101074\n",
      "Epoch: 12500 cost= 625.649902344\n",
      "Epoch: 13000 cost= 731.614135742\n",
      "Epoch: 13500 cost= 697.470703125\n",
      "Epoch: 14000 cost= 957.274169922\n",
      "Epoch: 14500 cost= 334.106262207\n",
      "Epoch: 15000 cost= 538.670043945\n",
      "Epoch: 15500 cost= 348.913177490\n",
      "Epoch: 16000 cost= 490.320770264\n",
      "Epoch: 16500 cost= 419.248779297\n",
      "Epoch: 17000 cost= 549.827636719\n",
      "Epoch: 17500 cost= 207.758346558\n",
      "Epoch: 18000 cost= 295.161590576\n",
      "Epoch: 18500 cost= 208.078704834\n",
      "Epoch: 19000 cost= 399.157043457\n",
      "Epoch: 19500 cost= 225.120971680\n",
      "Epoch: 20000 cost= 306.122375488\n",
      "Epoch: 20500 cost= 154.314331055\n",
      "Epoch: 21000 cost= 162.837951660\n",
      "Epoch: 21500 cost= 117.771858215\n",
      "Epoch: 22000 cost= 190.965972900\n",
      "Epoch: 22500 cost= 116.195526123\n",
      "Epoch: 23000 cost= 170.660568237\n",
      "Epoch: 23500 cost= 107.515563965\n",
      "Epoch: 24000 cost= 91.481414795\n",
      "Epoch: 24500 cost= 91.769180298\n",
      "Epoch: 25000 cost= 82.840087891\n",
      "Epoch: 25500 cost= 69.955642700\n",
      "Epoch: 26000 cost= 111.554824829\n",
      "Epoch: 26500 cost= 45.735645294\n",
      "Epoch: 27000 cost= 65.317398071\n",
      "Epoch: 27500 cost= 36.941059113\n",
      "Epoch: 28000 cost= 52.942001343\n",
      "Epoch: 28500 cost= 50.427276611\n",
      "Epoch: 29000 cost= 83.740226746\n",
      "Epoch: 29500 cost= 50.992347717\n",
      "Epoch: 30000 cost= 31.854789734\n",
      "Epoch: 30500 cost= 33.751129150\n",
      "Epoch: 31000 cost= 37.427017212\n",
      "Epoch: 31500 cost= 40.769840240\n",
      "Epoch: 32000 cost= 51.648468018\n",
      "Epoch: 32500 cost= 50.266460419\n",
      "Epoch: 33000 cost= 23.945075989\n",
      "Epoch: 33500 cost= 47.532638550\n",
      "Epoch: 34000 cost= 34.075538635\n",
      "Epoch: 34500 cost= 29.210660934\n",
      "Epoch: 35000 cost= 66.754707336\n",
      "Epoch: 35500 cost= 50.267898560\n",
      "Epoch: 36000 cost= 19.848424911\n",
      "Epoch: 36500 cost= 30.265676498\n",
      "Epoch: 37000 cost= 30.348075867\n",
      "Epoch: 37500 cost= 30.834688187\n",
      "Epoch: 38000 cost= 42.070568085\n",
      "Epoch: 38500 cost= 52.046089172\n",
      "Epoch: 39000 cost= 21.696283340\n",
      "Epoch: 39500 cost= 29.391517639\n",
      "Epoch: 40000 cost= 21.442035675\n",
      "Training loss= 34.092793 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        i_batch = (epoch % n_batch)*mini_batch_size\n",
    "        batch = X_train[i_batch:i_batch+mini_batch_size], y_train[i_batch:i_batch+mini_batch_size]\n",
    "        gs = sess.run(grads, feed_dict={X: batch[0], Y: np.reshape(batch[1], (-1,1))})  # This line is not needed for\n",
    "        print(gs)                                                                       # optimization, just for visualization\n",
    "        \n",
    "        sess.run(application, feed_dict={X: batch[0], Y: np.reshape(batch[1], (-1,1))})\n",
    "        \n",
    "        if (epoch+1) % display_step == 0:\n",
    "            err, p = sess.run((loss, tf.nn.sigmoid(prediction)), feed_dict={X: batch[0],  Y: np.reshape(batch[1], (-1,1))})\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(err))\n",
    "\n",
    "    training_loss = sess.run(loss, feed_dict={X: X_test, Y: np.reshape(y_test, (-1,1))})\n",
    "    print(\"Training loss=\", training_loss, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "Now, we are going to simulate the backpropagation algorithm. As you know, this algorithm computes the gradients (how the weights should change) of the network starting from the last layer to the first one. For that, we have to accomplish the following objectives:\n",
    "\n",
    "1) Create a deep neural network with more than one hidden layer. Keep the number of neurons relatively low, so that the training of the model does not consume much time.\n",
    "\n",
    "2) Use the *compute_gradients* function to compute the gradients of the layers. In this exercise, this operation must be done separately for each layer; you need to call the *compute_gradients* function for each layer of the network. Use the *var_list* parameter of the function.\n",
    "\n",
    "3) Use the *apply_gradients* function as it needs to be used, to create the tensorflow operations that we will later use to update the weights.\n",
    "\n",
    "4) Modify the training algorithm. Include in this algorithm the gradient application operations that you created.\n",
    "\n",
    "Take into account the order in which these operatinos have to be run by tensorflow, maintaining coherence with the backpropagation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "mini_batch_size = 100\n",
    "\n",
    "X = tf.placeholder(\"float\", shape=[None, 784], name=\"X\")\n",
    "Y = tf.placeholder(\"float\", shape=[None, 1], name=\"y\")\n",
    "\n",
    "\n",
    "# Declare the rest of variables\n",
    "\n",
    "\n",
    "\n",
    "# Create the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "\n",
    "adam = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "optimizer = adam.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "training_epochs = 40000\n",
    "display_step = 500\n",
    "perm = np.random.permutation(train_n_samples)\n",
    "\n",
    "\n",
    "n_batch = train_n_samples // mini_batch_size + (train_n_samples % mini_batch_size != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the compute_gradients and apply_gradient operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0500 cost= 31113.199218750\n",
      "Epoch: 1000 cost= 8554.460937500\n",
      "Epoch: 1500 cost= 8937.524414062\n",
      "Epoch: 2000 cost= 14167.705078125\n",
      "Epoch: 2500 cost= 5528.160156250\n",
      "Epoch: 3000 cost= 6429.468750000\n",
      "Epoch: 3500 cost= 7346.627929688\n",
      "Epoch: 4000 cost= 3989.730712891\n",
      "Epoch: 4500 cost= 3643.077392578\n",
      "Epoch: 5000 cost= 6967.380859375\n",
      "Epoch: 5500 cost= 2028.039672852\n",
      "Epoch: 6000 cost= 3920.076171875\n",
      "Epoch: 6500 cost= 3830.023437500\n",
      "Epoch: 7000 cost= 2369.577392578\n",
      "Epoch: 7500 cost= 2162.732666016\n",
      "Epoch: 8000 cost= 3556.782470703\n",
      "Epoch: 8500 cost= 1083.005493164\n",
      "Epoch: 9000 cost= 2510.074462891\n",
      "Epoch: 9500 cost= 2099.039550781\n",
      "Epoch: 10000 cost= 1259.440307617\n",
      "Epoch: 10500 cost= 1269.145141602\n",
      "Epoch: 11000 cost= 1929.218383789\n",
      "Epoch: 11500 cost= 591.718566895\n",
      "Epoch: 12000 cost= 1578.470947266\n",
      "Epoch: 12500 cost= 1373.240356445\n",
      "Epoch: 13000 cost= 687.904907227\n",
      "Epoch: 13500 cost= 853.590148926\n",
      "Epoch: 14000 cost= 1093.875488281\n",
      "Epoch: 14500 cost= 343.560974121\n",
      "Epoch: 15000 cost= 973.454467773\n",
      "Epoch: 15500 cost= 970.135803223\n",
      "Epoch: 16000 cost= 433.560119629\n",
      "Epoch: 16500 cost= 672.186584473\n",
      "Epoch: 17000 cost= 592.134643555\n",
      "Epoch: 17500 cost= 190.186233521\n",
      "Epoch: 18000 cost= 621.644775391\n",
      "Epoch: 18500 cost= 621.378173828\n",
      "Epoch: 19000 cost= 297.951202393\n",
      "Epoch: 19500 cost= 386.595825195\n",
      "Epoch: 20000 cost= 353.094909668\n",
      "Epoch: 20500 cost= 123.198394775\n",
      "Epoch: 21000 cost= 387.095397949\n",
      "Epoch: 21500 cost= 398.942901611\n",
      "Epoch: 22000 cost= 188.282699585\n",
      "Epoch: 22500 cost= 277.469635010\n",
      "Epoch: 23000 cost= 241.271835327\n",
      "Epoch: 23500 cost= 112.116012573\n",
      "Epoch: 24000 cost= 233.032104492\n",
      "Epoch: 24500 cost= 257.644592285\n",
      "Epoch: 25000 cost= 114.204650879\n",
      "Epoch: 25500 cost= 209.301269531\n",
      "Epoch: 26000 cost= 170.791778564\n",
      "Epoch: 26500 cost= 66.360412598\n",
      "Epoch: 27000 cost= 152.598098755\n",
      "Epoch: 27500 cost= 151.753570557\n",
      "Epoch: 28000 cost= 76.947204590\n",
      "Epoch: 28500 cost= 127.295959473\n",
      "Epoch: 29000 cost= 112.445159912\n",
      "Epoch: 29500 cost= 48.097179413\n",
      "Epoch: 30000 cost= 115.722259521\n",
      "Epoch: 30500 cost= 86.661643982\n",
      "Epoch: 31000 cost= 57.604267120\n",
      "Epoch: 31500 cost= 86.123428345\n",
      "Epoch: 32000 cost= 80.883255005\n",
      "Epoch: 32500 cost= 43.492786407\n",
      "Epoch: 33000 cost= 96.902565002\n",
      "Epoch: 33500 cost= 57.130928040\n",
      "Epoch: 34000 cost= 65.977005005\n",
      "Epoch: 34500 cost= 74.745155334\n",
      "Epoch: 35000 cost= 65.294448853\n",
      "Epoch: 35500 cost= 29.514560699\n",
      "Epoch: 36000 cost= 67.121871948\n",
      "Epoch: 36500 cost= 52.093120575\n",
      "Epoch: 37000 cost= 44.756118774\n",
      "Epoch: 37500 cost= 73.636169434\n",
      "Epoch: 38000 cost= 59.904552460\n",
      "Epoch: 38500 cost= 34.858978271\n",
      "Epoch: 39000 cost= 67.732841492\n",
      "Epoch: 39500 cost= 46.056713104\n",
      "Epoch: 40000 cost= 48.550159454\n",
      "Training loss= 60.438225 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        i_batch = (epoch % n_batch)*mini_batch_size\n",
    "        batch = X_train[i_batch:i_batch+mini_batch_size], y_train[i_batch:i_batch+mini_batch_size]\n",
    "            \n",
    "        # Run applications\n",
    "        \n",
    "        if (epoch+1) % display_step == 0:\n",
    "            err, p = sess.run((loss, tf.nn.sigmoid(prediction)), feed_dict={X: batch[0],  Y: np.reshape(batch[1], (-1,1))})\n",
    "            \n",
    "            # Compute and show the accuracy of the model\n",
    "            \n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(err))\n",
    "\n",
    "    training_loss = sess.run(loss, feed_dict={X: X_test, Y: np.reshape(y_test, (-1,1))})\n",
    "    print(\"Training loss=\", training_loss, '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "**This exercise is optional, and, if completed, should be uploaded to egela.**\n",
    "\n",
    "We can, however, dig further into the computation of the gradients. For this exercise, the usage of the *tf.compute_graidents* is banned. Instead of that function, we will have to use the *tf.gradients* function, which computes the gradients. In this case, we are not making use of the optimizers implemented in tensorflow. Therefore, we will need to perform the optimization on our own.\n",
    "\n",
    "1) Use the *tf.gradient* function (https://www.tensorflow.org/api_docs/python/tf/gradients) to declare an operation that computes the gradients of the variables.\n",
    "\n",
    "2) Use the *tf.assign* function (https://www.tensorflow.org/api_docs/python/tf/assign) to create an operation that assigns certain value to a variable. In our case, the weights and biases of the model.\n",
    "\n",
    "3) Modify the training loop so that it can train the model using the defined operations, instead of those that use the tensorflow optimizers.\n",
    "\n",
    "4) Modify the algorithm so that it implements learning_rate decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "mini_batch_size = 100\n",
    "\n",
    "# Declare placeholders\n",
    "\n",
    "# Declare the rest of variables\n",
    "\n",
    "# Create the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "training_epochs = 40000\n",
    "display_step = 500\n",
    "perm = np.random.permutation(train_n_samples)\n",
    "\n",
    "\n",
    "n_batch = train_n_samples // mini_batch_size + (train_n_samples % mini_batch_size != 0)\n",
    "\n",
    "display_step = 500\n",
    "\n",
    "gs = tf.gradients(loss, variable_list)\n",
    "\n",
    "learning_rate = tf.Variable(0.00001)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of variables that need to be updated (first without learning_rate decay, then with decay)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0500 cost= 2643.220947266\n",
      "Epoch: 1000 cost= 772.092407227\n",
      "Epoch: 1500 cost= 983.369201660\n",
      "Epoch: 2000 cost= 405.895874023\n",
      "Epoch: 2500 cost= 193.922836304\n",
      "Epoch: 3000 cost= 339.877197266\n",
      "Epoch: 3500 cost= 187.692291260\n",
      "Epoch: 4000 cost= 389.246246338\n",
      "Epoch: 4500 cost= 287.942840576\n",
      "Epoch: 5000 cost= 277.470214844\n",
      "Epoch: 5500 cost= 146.926696777\n",
      "Epoch: 6000 cost= 462.152618408\n",
      "Epoch: 6500 cost= 200.870971680\n",
      "Epoch: 7000 cost= 292.698394775\n",
      "Epoch: 7500 cost= 339.680358887\n",
      "Epoch: 8000 cost= 286.861572266\n",
      "Epoch: 8500 cost= 202.637817383\n",
      "Epoch: 9000 cost= 154.537170410\n",
      "Epoch: 9500 cost= 189.646255493\n",
      "Epoch: 10000 cost= 249.607971191\n",
      "Epoch: 10500 cost= 290.993347168\n",
      "Epoch: 11000 cost= 378.125854492\n",
      "Epoch: 11500 cost= 131.790023804\n",
      "Epoch: 12000 cost= 188.687072754\n",
      "Epoch: 12500 cost= 255.569412231\n",
      "Epoch: 13000 cost= 204.271820068\n",
      "Epoch: 13500 cost= 255.360778809\n",
      "Epoch: 14000 cost= 633.768920898\n",
      "Epoch: 14500 cost= 80.204093933\n",
      "Epoch: 15000 cost= 183.813385010\n",
      "Epoch: 15500 cost= 249.433731079\n",
      "Epoch: 16000 cost= 294.091491699\n",
      "Epoch: 16500 cost= 222.498840332\n",
      "Epoch: 17000 cost= 256.769012451\n",
      "Epoch: 17500 cost= 81.176856995\n",
      "Epoch: 18000 cost= 80.523406982\n",
      "Epoch: 18500 cost= 161.988830566\n",
      "Epoch: 19000 cost= 200.597579956\n",
      "Epoch: 19500 cost= 270.096557617\n",
      "Epoch: 20000 cost= 461.191955566\n",
      "Epoch: 20500 cost= 106.954383850\n",
      "Epoch: 21000 cost= 114.122825623\n",
      "Epoch: 21500 cost= 262.319274902\n",
      "Epoch: 22000 cost= 267.599639893\n",
      "Epoch: 22500 cost= 193.781265259\n",
      "Epoch: 23000 cost= 149.219848633\n",
      "Epoch: 23500 cost= 48.893028259\n",
      "Epoch: 24000 cost= 141.209838867\n",
      "Epoch: 24500 cost= 201.067825317\n",
      "Epoch: 25000 cost= 212.601196289\n",
      "Epoch: 25500 cost= 182.300765991\n",
      "Epoch: 26000 cost= 152.460266113\n",
      "Epoch: 26500 cost= 56.137363434\n",
      "Epoch: 27000 cost= 109.935089111\n",
      "Epoch: 27500 cost= 216.206604004\n",
      "Epoch: 28000 cost= 134.042251587\n",
      "Epoch: 28500 cost= 159.526763916\n",
      "Epoch: 29000 cost= 200.322128296\n",
      "Epoch: 29500 cost= 107.127403259\n",
      "Epoch: 30000 cost= 82.053245544\n",
      "Epoch: 30500 cost= 128.122299194\n",
      "Epoch: 31000 cost= 140.492233276\n",
      "Epoch: 31500 cost= 184.880584717\n",
      "Epoch: 32000 cost= 132.353942871\n",
      "Epoch: 32500 cost= 123.187774658\n",
      "Epoch: 33000 cost= 106.222808838\n",
      "Epoch: 33500 cost= 137.482192993\n",
      "Epoch: 34000 cost= 150.081115723\n",
      "Epoch: 34500 cost= 207.348236084\n",
      "Epoch: 35000 cost= 159.420028687\n",
      "Epoch: 35500 cost= 69.017120361\n",
      "Epoch: 36000 cost= 81.491767883\n",
      "Epoch: 36500 cost= 72.579444885\n",
      "Epoch: 37000 cost= 107.131614685\n",
      "Epoch: 37500 cost= 153.520141602\n",
      "Epoch: 38000 cost= 550.467041016\n",
      "Epoch: 38500 cost= 46.497432709\n",
      "Epoch: 39000 cost= 73.876281738\n",
      "Epoch: 39500 cost= 117.193756104\n",
      "Epoch: 40000 cost= 143.985275269\n",
      "Training loss= 417.19882 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Optimized solution\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        i_batch = (epoch % n_batch)*mini_batch_size\n",
    "        batch = X_train[i_batch:i_batch+mini_batch_size], y_train[i_batch:i_batch+mini_batch_size]\n",
    "        # Run updates\n",
    "            \n",
    "        if (epoch+1) % display_step == 0:\n",
    "            err, p = sess.run((loss, tf.nn.sigmoid(prediction)), feed_dict={X: batch[0],  Y: np.reshape(batch[1], (-1,1))})\n",
    "            \n",
    "            # Compute and show the accuracy of the model\n",
    "            \n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(err))\n",
    "\n",
    "    training_loss = sess.run(loss, feed_dict={X: X_test, Y: np.reshape(y_test, (-1,1))})\n",
    "    print(\"Training loss=\", training_loss, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
